{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GW18MExyoByK"
   },
   "source": [
    "## <font color=red> You should not import any new libraries. Your code should run with python=3.x</font>\n",
    "## <font color=red> Please don't rename this .ipynb file.</font><br>\n",
    "- Your solutions will be auto-graded. Hence we request you to follow the instructions.\n",
    "- Modify the code only between \n",
    "```\n",
    "## TODO\n",
    "## END TODO\n",
    "```\n",
    "- In addition to above changes, you can play with arguments to the functions for generating plots\n",
    "- We will run the auto grading scripts with private test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aCSR1aBfmVPu"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "from scipy import linalg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g7zKGgQgoPnA"
   },
   "source": [
    "## Please make sure that your code works with loading data from relative path only\n",
    "\n",
    "i.e. ```pd.read_csv('./data/multi_var_lasso.csv')``` should not throw an error when we run the auto-grading scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xU2CdrwCoQQv"
   },
   "outputs": [],
   "source": [
    "data_multi = pd.read_csv('./data/multi_var_lasso.csv')\n",
    "cols = [f\"x_gt_{idx}\" for idx in range(1, 6)]\n",
    "X_multi = np.array(data_multi[cols])\n",
    "Y_multi = np.array(data_multi['y_gt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j_YZg7Z7g3fl"
   },
   "outputs": [],
   "source": [
    "data_multi_class = pd.read_csv('./data/3_class_perceptron.csv')\n",
    "cols = [f\"x_gt_{idx}\" for idx in range(1, 3)]\n",
    "X_multi_class = np.array(data_multi_class[cols])\n",
    "Y_multi_class = np.array(data_multi_class['y_gt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0DaZIrqza4oc"
   },
   "outputs": [],
   "source": [
    "def mse_multi_var(X, Y, w, b):\n",
    "    '''\n",
    "    Compute mean squared error between predictions and true y values\n",
    "\n",
    "    Args:\n",
    "    X - numpy array of shape (n_samples, 1)\n",
    "    Y - numpy array of shape (n_samples, 1)\n",
    "    w - a float\n",
    "    b - a float\n",
    "    '''\n",
    "\n",
    "    ## TODO\n",
    "    N = X.shape[0]\n",
    "    mse = np.square(np.dot(X, w) + b - Y).mean() / N\n",
    "\n",
    "    ## END TODO\n",
    "\n",
    "    return mse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B6AFlQQWoxSh"
   },
   "outputs": [],
   "source": [
    "def mse_regularized(X, Y, w, b, lamda):\n",
    "    '''\n",
    "    Compute mean squared error between predictions and true y values\n",
    "\n",
    "    Args:\n",
    "    X - numpy array of shape (n_samples, 1)\n",
    "    Y - numpy array of shape (n_samples, 1)\n",
    "    w - a float\n",
    "    b - a float\n",
    "    '''\n",
    "    ## TODO\n",
    "    N = X.shape[0]\n",
    "    mse = lamda * np.square(w).mean() + \\\n",
    "        np.square(np.dot(X, w) + b - Y).mean() / N\n",
    "\n",
    "    ## END TODO\n",
    "\n",
    "    return mse\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UyT4gundogZ5"
   },
   "source": [
    "## Plot Graphs\n",
    "\n",
    "- This function plots the ground truth curve in <font color=green>green</font> and the predicted function in <font color=red>red</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "weGoGCzJodzM"
   },
   "outputs": [],
   "source": [
    "def plot_curves(w, b, x, y):\n",
    "    x_gt = np.linspace(-1, 2, 50)\n",
    "    y_gt = 1 - 3 * x_gt - 2 * x_gt ** 2 + 2.5 * x_gt ** 3\n",
    "    # print(x_gt.shape,y_gt.shape)\n",
    "    if len(w) == 1:\n",
    "        y_fit = w * x_gt + b\n",
    "    elif len(w) == 5:\n",
    "        x_fit = x_gt\n",
    "        for pow in range(2, 4):\n",
    "            x_fit = np.vstack([x_fit, np.power(x_gt, pow)])\n",
    "\n",
    "        x_fit = np.vstack([x_fit, np.power(x_gt, 2)])\n",
    "        x_fit = np.vstack([x_fit, np.power(x_gt, 1)])\n",
    "\n",
    "        y_fit = np.dot(w, x_fit) + b\n",
    "    else:\n",
    "        assert False, 'Pass a valid w'\n",
    "    plt.plot(x_gt, y_gt, color=\"green\", label='1 - 3 * x - 2 * x ** 2 + 2.5 * x ** 3')\n",
    "    plt.plot(x_gt, y_fit, color='red', label=\"Fitted Function y = w.Tx + b\")\n",
    "    if len(x.shape) == 1:\n",
    "        x_plot = np.vstack([x, np.ones(len(x))]).T\n",
    "        # print (x_plot.shape, y.shape)\n",
    "    else:\n",
    "        x_plot = x\n",
    "    plt.scatter(x_plot[:, 0], y)\n",
    "    plt.legend()\n",
    "    plt.title(\"Ground Truth Function\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-oAg-WHfo_j-"
   },
   "outputs": [],
   "source": [
    "def split_data(X, Y, train_ratio=0.6):\n",
    "    '''\n",
    "    Split data into train and validation sets such that train\n",
    "    contains floor(n_sampes * train_ratio) and test contains the remaining\n",
    "    samples\n",
    "\n",
    "    Args:\n",
    "    X - numpy array of shape (n_samples, n_features)\n",
    "    Y - numpy array of shape (n_samples, 1)\n",
    "    train_ratio - fraction of samples to be used as training data\n",
    "\n",
    "    Returns:\n",
    "    X_train, Y_train, X_val, Y_val\n",
    "    '''\n",
    "    ## TODO\n",
    "    n_samples = X.shape[0]\n",
    "    train_size = int(n_samples * train_ratio)\n",
    "\n",
    "    np.random.seed(0)\n",
    "    indices = np.random.permutation(n_samples)\n",
    "    X_train = X[indices[:train_size]]\n",
    "    Y_train = Y[indices[:train_size]]\n",
    "    X_val = X[indices[train_size:]]\n",
    "    Y_val = Y[indices[train_size:]]\n",
    "\n",
    "    ## END TODO\n",
    "\n",
    "    return X_train, Y_train, X_val, Y_val\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jF5le-PSpHyI"
   },
   "source": [
    "# Lasso Regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KXHA6LqRpJOL"
   },
   "outputs": [],
   "source": [
    "X_train, Y_train, X_val, Y_val = split_data(X_multi, Y_multi, train_ratio=0.6)\n",
    "\n",
    "\n",
    "def soft(x, threshold):\n",
    "    ind_high = x > threshold\n",
    "    ind_low = x < -threshold\n",
    "    res = np.zeros_like(x)\n",
    "    res[ind_high] = x[ind_high] - threshold\n",
    "    res[ind_low] = x[ind_low] + threshold\n",
    "    return res\n",
    "\n",
    "\n",
    "def ista(X_train, Y_train, X_val, Y_val, epochs=100, lr=1e-3, lamda=1):\n",
    "    '''\n",
    "    Perform multi variable lasso regression using ISTA\n",
    "\n",
    "    Args:\n",
    "    X_train - numpy array of shape (n_samples_train, 5)\n",
    "    Y_train - numpy array of shape (n_samples_train, 1)\n",
    "    X_val - numpy array of shape (n_samples_val, 5)\n",
    "    Y_val - numpy array of shape (n_samples_val, 1)\n",
    "    epochs - number of gradient descent steps\n",
    "    lr - learnig rate\n",
    "    lamda - regularization_weight\n",
    "    '''\n",
    "\n",
    "    w = np.zeros(X_train.shape[1])  # np.random.randn(X_train.shape[1])\n",
    "    b = 0\n",
    "    ## TODO\n",
    "    N = X_train.shape[0]\n",
    "    for i in range(epochs):\n",
    "        grad_w = 2 / N * np.dot(np.dot(X_train, w) + b - Y_train, X_train)\n",
    "        grad_b = 2 / N * np.sum(np.dot(X_train, w) + b - Y_train)\n",
    "        w_LS = w - lr * grad_w\n",
    "        b_LS = b - lr * grad_b\n",
    "        w = soft(w_LS, lr * lamda)\n",
    "        b = soft(b_LS, lr * lamda)\n",
    "        print(w, w_LS)\n",
    "\n",
    "    ## END TODO\n",
    "\n",
    "    mse_train = mse_regularized(X_train, Y_train, w, b, lamda)\n",
    "    mse_val = mse_regularized(X_val, Y_val, w, b, lamda)\n",
    "    print(f'Validation loss if {mse_val}')\n",
    "    print(f'Training Loss loss if {mse_train}')\n",
    "    print(w.shape, b, X_train.shape, Y_train.shape)\n",
    "    plot_curves(list(w), b, X_train, Y_train)\n",
    "    return w, b\n",
    "\n",
    "\n",
    "ista(X_train, Y_train, X_val, Y_val)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tquhPbDFVo5J"
   },
   "source": [
    "# Ridge Regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mBD7ZLePVhFz"
   },
   "outputs": [],
   "source": [
    "def multivar_reg_closedform(X_train, Y_train, X_val, Y_val, lamda=0.5):\n",
    "    '''\n",
    "    Perform L2 regularized multi variable least squares regression using \n",
    "    closed form update rules\n",
    "\n",
    "    Args:\n",
    "    X_train - numpy array of shape (n_samples_train, 5)\n",
    "    Y_train - numpy array of shape (n_samples_train, 1)\n",
    "    X_val - numpy array of shape (n_samples_val, 5)\n",
    "    Y_val - numpy array of shape (n_samples_val, 1)\n",
    "    lambda - regularization weight\n",
    "    '''\n",
    "\n",
    "    w = np.zeros(X_train.shape[1])\n",
    "    b = 0\n",
    "\n",
    "    ## TODO\n",
    "    # use closed-form solution from previous assignment\n",
    "    N = X_train.shape[0]\n",
    "    phi = np.hstack((np.ones((N, 1)), X_train))\n",
    "    weight = lamda * N * np.eye(X_train.shape[1] + 1)\n",
    "    weight[0, 0] = 0\n",
    "    w_all = linalg.inv(phi.T @ phi + weight) @ phi.T @ Y_train\n",
    "    w = w_all[1:]\n",
    "    b = w_all[0]\n",
    "\n",
    "    ## END TODO\n",
    "\n",
    "    mse_train = mse_multi_var(X_train, Y_train, w, b)\n",
    "    mse_val = mse_multi_var(X_val, Y_val, w, b)\n",
    "    print(f'Validation loss if {mse_val}')\n",
    "    print(f'Training Loss loss if {mse_train}')\n",
    "    plot_curves(list(w), b, X_train, Y_train)\n",
    "\n",
    "    return w, b\n",
    "\n",
    "\n",
    "multivar_reg_closedform(X_train, Y_train, X_val, Y_val)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HafXtfE0Zx6D"
   },
   "source": [
    "# Bias-Variance Tradeoff\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aBatCgQgeZi8"
   },
   "outputs": [],
   "source": [
    "#\n",
    "def ridge(X_train, Y_train, X_test, epochs=20, lr=2e-1, lamda=1):\n",
    "\n",
    "    w = 0.6\n",
    "    b = 2\n",
    "    n = float(len(X_train))  # Number of elements in X\n",
    "\n",
    "    # Performing Gradient Descent\n",
    "    ## TODO\n",
    "\n",
    "    ## END TODO\n",
    "\n",
    "    Y_pred = w*X_test+b\n",
    "    return Y_pred\n",
    "\n",
    "\n",
    "def lasso(X_train, Y_train, X_test, epochs=20, lr=2e-1, lamda=1):\n",
    "    w = 0.6\n",
    "    b = 2\n",
    "    n = float(len(X_train))  # Number of elements in X\n",
    "\n",
    "    # Performing Gradient Descent\n",
    "    ## TODO\n",
    "\n",
    "    ## END TODO\n",
    "\n",
    "    Y_pred = w*X_test+b\n",
    "    return Y_pred\n",
    "\n",
    "\n",
    "def ols(X_train, Y_train, X_test, epochs=20, lr=2e-1):\n",
    "\n",
    "    w = 0.6\n",
    "    b = 2\n",
    "    n = float(len(X_train))  # Number of elements in X\n",
    "\n",
    "    # Performing Gradient Descent\n",
    "    ## TODO\n",
    "\n",
    "    ## END TODO\n",
    "\n",
    "    Y_pred = w*X_test+b\n",
    "    return Y_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "78u8LgPFZxZ0"
   },
   "outputs": [],
   "source": [
    "# We fit multiple lines onto our linear model defined by y = c + mx + error.\n",
    "num_lines = 10\n",
    "# size of Dataset, len(Dataset) for 1 line\n",
    "n = 500\n",
    "# weights (slope/intercept)\n",
    "c = 3\n",
    "m = 5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mjwhG142Z6pL"
   },
   "outputs": [],
   "source": [
    "def gen_data():\n",
    "    '''\n",
    "    We sample n, X from uniform distribution and error from zero mean normal distribtion, we find Y using mx+c+e\n",
    "    We do the above for num_lines number of time, to fit different lines for each models(lasso, ols, ridge), doing so will\n",
    "    be helpfull in calculating Expected value of learned esitmator of all n*num_lines dataset\n",
    "\n",
    "    DO NOT CHANGE this function.\n",
    "    '''\n",
    "\n",
    "    w0 = c\n",
    "    w1 = m\n",
    "    data = np.zeros(shape=(num_lines, n, 2))\n",
    "    for i in range(num_lines):\n",
    "        x = np.random.uniform(-2, 2, size=(n, 1))\n",
    "        e = np.random.normal(0, 8, size=(n, 1))\n",
    "        y = w0 + w1 * x + e\n",
    "        x_y = np.concatenate((x, y), axis=1)\n",
    "        data[i, :, :] = x_y\n",
    "    return np.array(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KXpyZZMLZ-wk"
   },
   "outputs": [],
   "source": [
    "def gen_bais_variance(Y_preds, Y_true):\n",
    "    ## TODO\n",
    "\n",
    "    ## END TODO\n",
    "    return bias, variance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hMzoObAiaBTX"
   },
   "outputs": [],
   "source": [
    "def make_prediction(data, X_test, lambda_=0.5):\n",
    "\n",
    "    y_hat = np.zeros(shape=(num_lines, 3))  # store prediction of all model\n",
    "\n",
    "    for i in range(num_lines):\n",
    "        X = data[i, :, 0].reshape(n, 1)\n",
    "        y = data[i, :, 1].reshape(n, 1)\n",
    "\n",
    "        y_hat[i, 0] = ols(X, y, X_test)\n",
    "\n",
    "        y_hat[i, 1] = ridge(X, y, X_test, lamda=lambda_)\n",
    "\n",
    "        y_hat[i, 2] = lasso(X, y, X_test, lamda=lambda_)\n",
    "\n",
    "    return y_hat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DinGJiTgaENE"
   },
   "outputs": [],
   "source": [
    "def plot_figure(l, b, v):\n",
    "\n",
    "    plt.plot(l, b[:, 0], label=\"OLS\")\n",
    "    plt.plot(l, b[:, 1], label=\"Ridge\")\n",
    "    plt.plot(l, b[:, 2], label=\"Lasso\")\n",
    "    plt.xlabel(\"Regularization coefficient\")\n",
    "    plt.ylabel(\"Bias\")\n",
    "    plt.title(\"Bias vs Lambda\")\n",
    "    plt.legend(loc=\"upper left\")\n",
    "    plt.show()\n",
    "\n",
    "    plt.plot(l, v[:, 0], label=\"OLS\")\n",
    "    plt.plot(l, v[:, 1], label=\"Ridge\")\n",
    "    plt.plot(l, v[:, 2], label=\"Lasso\")\n",
    "    plt.xlabel(\"Regularization coefficient\")\n",
    "    plt.ylabel(\"Variance\")\n",
    "    plt.title(\"Variance vs Lambda\")\n",
    "    plt.legend(loc=\"lower left\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nJD2hCXxaIAA"
   },
   "outputs": [],
   "source": [
    "def driver():\n",
    "    all_lambda = np.linspace(0.0001, 1, 100)\n",
    "    all_bias = np.zeros(shape=(len(all_lambda), 3))\n",
    "    all_variance = np.zeros(shape=(len(all_lambda), 3))\n",
    "    dataset = gen_data()\n",
    "    X_test = np.array([4]).reshape((1, 1))\n",
    "    for i, l in enumerate(all_lambda):\n",
    "        Y_hat = make_prediction(dataset, X_test, lambda_=l)\n",
    "        ## TODO\n",
    "        # calculate true mean Y_true\n",
    "\n",
    "        ## END TODO\n",
    "        all_bias[i, :], all_variance[i, :] = gen_bais_variance(Y_hat, Y_true)\n",
    "    plot_figure(all_lambda, all_bias, all_variance)\n",
    "\n",
    "\n",
    "driver()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bo75okrxU6D0"
   },
   "source": [
    "# Perceptron\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QaL7Zo3nU8Vm"
   },
   "outputs": [],
   "source": [
    "X_train_multiclass, Y_train_multiclass, X_val_multiclass, Y_val_multiclass = split_data(\n",
    "    X_multi_class, Y_multi_class, train_ratio=0.6)\n",
    "\n",
    "\n",
    "def perceptron_algo(X_train_multiclass, Y_train_multiclass, X_val_multiclass, Y_val_multiclass, n_class=3, epochs=30):\n",
    "    W = np.zeros((n_class, len(X_train_multiclass[0])))\n",
    "    ## TODO\n",
    "\n",
    "    ## END TODO\n",
    "\n",
    "    Y_pred = list()\n",
    "    for i in range(len(Y_val_multiclass)):\n",
    "        y_pred = np.argmax([np.dot(W[0], X_val_multiclass[i]), np.dot(\n",
    "            W[1], X_val_multiclass[i]), np.dot(W[2], X_val_multiclass[i])])\n",
    "        Y_pred.append(y_pred)\n",
    "\n",
    "    from sklearn.metrics import classification_report\n",
    "    target_names = ['class 0', 'class 1', 'class 2']\n",
    "    print(classification_report(Y_val_multiclass, Y_pred, target_names=target_names))\n",
    "    return W\n",
    "\n",
    "\n",
    "perceptron_algo(X_train_multiclass, Y_train_multiclass, X_val_multiclass, Y_val_multiclass)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Assignment_2.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "01ecde67ca3c6c9b16e86f11e0c33349bd2057703bfde9df3101ec0e6a0242be"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
